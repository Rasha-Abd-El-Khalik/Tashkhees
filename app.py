import gradio as gr
import speech_recognition as sr
from gtts import gTTS
import re, torch
from camel_tools.utils.normalize import normalize_alef_maksura_ar, normalize_alef_ar, normalize_teh_marbuta_ar
from camel_tools.utils.dediac import dediac_ar
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from peft import PeftModel
from langchain_google_genai import ChatGoogleGenerativeAI

# === MARBERT ===
tokenizer = AutoTokenizer.from_pretrained("./marbert_finetuned_lora")
base_model = AutoModelForSequenceClassification.from_pretrained("UBC-NLP/MARBERT", num_labels=5)
model = PeftModel.from_pretrained(base_model, "./marbert_finetuned_lora")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()


llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",  
    google_api_key="AIzaSyCYJyangnXI9r_MiOf-6GTXgyz804Oi2II",
)


def camel_clean(text):
    text = dediac_ar(str(text))
    text = normalize_alef_ar(text)
    text = normalize_alef_maksura_ar(text)
    text = normalize_teh_marbuta_ar(text)
    en2ar = str.maketrans('0123456789', 'Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©')
    text = text.translate(en2ar)
    text = re.sub(r'[^\u0600-\u06FF\s.,;:!?()\'"-]', '', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip().lower()


def speech_to_text(audio_file):
    recognizer = sr.Recognizer()
    with sr.AudioFile(audio_file) as source:
        audio_data = recognizer.record(source)
        try:
            text = recognizer.recognize_google(audio_data, language="ar-EG")
        except:
            text = "âš ï¸ ØªØ¹Ø°Ø± Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØªØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø¬Ø¯Ø¯Ù‹Ø§"
    return text

def predict_text(text):
    inputs = tokenizer(text, padding="max_length", truncation=True, max_length=128, return_tensors="pt")
    inputs = {k: v.to(device) for k, v in inputs.items()}
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=-1)
        max_prob, pred = torch.max(probs, dim=-1)
    return pred.item(), max_prob.item()


def map_class(pred):
    mapping = {
        0: "Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„ØªÙ†ÙØ³ÙŠ",
        1: "Ø§Ù„ØºØ¯Ø¯ Ø§Ù„ØµÙ…Ø§Ø¡",
        2: "Ø¬Ø±Ø§Ø­Ø© Ø§Ù„Ø¹Ø¸Ø§Ù…",
        3: "Ø¬Ø±Ø§Ø­Ø© Ø¹Ø§Ù…Ø©",
        4: "Ù…Ø±Ø¶ Ø§Ù„Ø³ÙƒØ±ÙŠ"
    }
    return mapping.get(pred, "Ø®Ø§Ø±Ø¬ Ø§Ù„ÙØ¦Ø§Øª")


def generate_category_with_gemini(question):
    prompt = f"""
Ø§Ø³ØªØ®Ø±Ø¬ ÙÙ‚Ø· Ø§Ù„ØªØ®ØµØµ Ø§Ù„Ø·Ø¨ÙŠ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨ÙƒÙ„Ù…Ø© Ø£Ùˆ ÙƒÙ„Ù…ØªÙŠÙ† ÙƒØ­Ø¯ Ø£Ù‚ØµÙ‰.
Ù…Ø«Ø§Ù„:
- "Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ù‚Ù„Ø¨"
- "Ø·Ø¨ Ø§Ù„Ø£Ø·ÙØ§Ù„"
- "Ø¬Ø±Ø§Ø­Ø© Ø¹Ø§Ù…Ø©"
- "Ø§Ù„Ø¹Ù†Ø§ÙŠØ© Ø¨Ø§Ù„Ø´Ø¹Ø±"
- "Ø§Ù„Ø³ÙƒØ±ÙŠ"

Ø§Ù„Ø³Ø¤Ø§Ù„: {question}
Ø§ÙƒØªØ¨ Ø§Ù„ÙØ¦Ø© ÙÙ‚Ø· Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ø´Ø±Ø­ Ø¥Ø¶Ø§ÙÙŠ.
"""
    response = llm.invoke(prompt)
    return response.content.strip()

def generate_answer_with_gemini(question, max_tokens=100):
    prompt = f"""
Ø£Ù†Øª Ø·Ø¨ÙŠØ¨ Ù…ØªØ®ØµØµ Ø°Ùˆ Ø®Ø¨Ø±Ø© Ø¹Ø§Ù„ÙŠØ©. Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø£ÙŠ Ø³Ø¤Ø§Ù„ Ø·Ø¨ÙŠ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø§Ø­ØªØ±Ø§ÙÙŠØ©ØŒ Ø¹Ù„Ù…ÙŠØ©ØŒ Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙˆØ§Ø¶Ø­Ø©:
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ ÙÙ‚Ø·. **Ù„Ø§ ØªØ¬Ø¨ Ø£Ø¨Ø¯Ù‹Ø§ Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©.**
- Ù„Ø§ ØªØªØ¬Ø§ÙˆØ² Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© 250 Ø­Ø±Ù.
- Ø±ÙƒÙ‘Ø² Ø¹Ù„Ù‰ Ø§Ù„ØªØ´Ø®ÙŠØµ Ø§Ù„Ù…Ø¨Ø¯Ø¦ÙŠ Ø£Ùˆ Ø§Ù„ØªÙˆØµÙŠØ© Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©.
- Ø£Ø¬Ø¨ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¯ÙˆÙ† Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø£Ùˆ Ø¥Ø¶Ø§ÙØ© ØªÙØ§ØµÙŠÙ„ ØºÙŠØ± Ø¶Ø±ÙˆØ±ÙŠØ©.
- Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø§Ø®ØªØµØ§Ø±Ø§Øª Ø£Ùˆ Ù…ØµØ·Ù„Ø­Ø§Øª ØºØ§Ù…Ø¶Ø©.

Ø§Ù„Ø³Ø¤Ø§Ù„: {question}
Ø£Ø¬Ø¨ Ø§Ù„Ø¢Ù† Ù…Ø¨Ø§Ø´Ø±Ø© ÙˆØ§Ø­ØªØ±Ø§ÙÙŠÙ‹Ø§ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·ØŒ **Ù„Ø§ ØªÙØ¬Ø¨ Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©**:
"""
    response = llm.invoke(prompt)
    return response.content

# === ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØª ===
def text_to_speech(text, filename="output.mp3"):
    if text.strip() == "":
        return None
    tts = gTTS(text=text, lang="ar")
    tts.save(filename)
    return filename

THRESHOLD = 0.90
def process(audio_file):
    if audio_file is None:
        return "â€”", None, "ğŸš« Ù„Ù… ÙŠØªÙ… Ø¥Ø¯Ø®Ø§Ù„ ØµÙˆØª", None

    clean = camel_clean(speech_to_text(audio_file))

    # MARBERT prediction
    pred, confidence = predict_text(clean)
    mapped = map_class(pred)

    if confidence >= THRESHOLD:
       
        category_text = mapped
        answer_text = generate_answer_with_gemini(clean)
    else:
  
        category_text = generate_category_with_gemini(clean)
        answer_text = generate_answer_with_gemini(clean)


    category_speech = text_to_speech(category_text, filename="category.mp3")
    answer_speech = text_to_speech(answer_text, filename="answer.mp3")

    return category_text, category_speech, answer_text, answer_speech

with gr.Blocks(css="""
    body, .gradio-container {min-height: 100vh; background: #f7f9fb; font-family: 'Cairo', sans-serif;}
    .header-bar, .footer-bar {width:100%; text-align:center; padding:15px; border-radius:12px;
                               background:linear-gradient(90deg, #00796b, #00acc1); color:white;
                               box-shadow:0 2px 10px rgba(0,0,0,0.15);}
    .footer-bar {margin-top:30px; font-size:0.85em; border-radius:0; padding:8px; box-shadow:none;}
    .header-text {font-size:1.8em; font-weight:bold; margin:0;}
    .sub-text {font-size:1em; color:#e0f2f1; margin-top:5px;}
    .card {border-radius:12px; padding:15px; background:#ffffff; color:#2c3e50;
           box-shadow:0px 4px 12px rgba(0,0,0,0.1);}
    .gr-textbox label, .gr-audio label {color:#00796b !important; font-weight:bold;}
""") as demo:

    # Header
    gr.HTML("""
    <div class="header-bar">
      <h1 class="header-text">ğŸ©º ØªØ´Ø®ÙŠØµ | Tashkhees</h1>
      <p class="sub-text">ÙˆØ§Ø¬Ù‡Ø© Ø°ÙƒÙŠØ© Ù„ØªØ­ÙˆÙŠÙ„ ÙƒÙ„Ø§Ù…Ùƒ Ø§Ù„Ø·Ø¨ÙŠ Ø¥Ù„Ù‰ ÙØ¦Ø© ÙˆØ¥Ø¬Ø§Ø¨Ø© Ù…Ø¨Ø§Ø´Ø±Ø©</p>
    </div>
    """)

    audio_input = gr.Audio(sources=["microphone"], type="filepath", label="ğŸ™ï¸ Ø³Ø¬Ù„ ØµÙˆØªÙƒ Ù‡Ù†Ø§")

    with gr.Row():
        with gr.Column():
            category_output = gr.Textbox(label="ğŸ·ï¸ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ù…ØµÙ†ÙØ©", interactive=False, elem_classes=["card"])
            category_audio = gr.Audio(label="ğŸ”Š Ø§Ù„ÙØ¦Ø© ØµÙˆØª", type="filepath")
        with gr.Column():
            answer_output = gr.Textbox(label="ğŸ’¡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©", interactive=False, lines=5, elem_classes=["card"])
            answer_audio = gr.Audio(label="ğŸ”Š Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ØµÙˆØª", type="filepath")

    # Footer
    gr.HTML("""
    <div class="footer-bar">
      <p>Â© 2025 ØªØ´Ø®ÙŠØµ | Tashkhees Project | Designed with â¤ï¸</p>
    </div>
    """)

    # Link function
    audio_input.change(fn=process, inputs=audio_input,
                       outputs=[category_output, category_audio, answer_output, answer_audio])

demo.launch(share=True)
